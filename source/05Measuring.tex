%*****************************************
\chapter{Defining and Measuring Concepts}\label{ch05:measuring}
%*****************************************
%TODO Status: Pre-draft

\section{Measurement}

Measurement is important. People who have attempted to bake a cake from scratch without measuring the ingredients will find, no doubt, that measurement is the difference between a happy birthday and a disaster. Just like in baking, measurement is important to a researcher. Measurement means the process by which key facts, attributes, concepts, and other phenomena are described. At its core, measurement is about defining the research project's terms in a precise and measurable way. Of course, measurement in business research is not quite as simple as using some predetermined or universally agreed-on tool, such as a measuring cup, but there are some basic tenants on which most researchers agree when it comes to measurement. 

\subsection{What Do Researchers Measure?}

The question of what business researchers measure can be answered by asking what business researchers study. Researchers study a wide variety of business and marketing concepts, like corporate culture\footnote{Denison, Daniel R. Corporate culture and organizational effectiveness. John Wiley \& Sons, 1990.}, the price elasticity of gasoline\footnote{Hughes, Jonathan E., Christopher R. Knittel, and Daniel Sperling. Evidence of a shift in the short-run price elasticity of gasoline demand. No. w12530. National Bureau of Economic Research, 2006.}, employee turnover\footnote{Hom, Peter W., and Rodger W. Griffeth. Employee turnover. South-Western Pub, 1995.}, and automobile ``lemons''\footnote{Akerlof, George A. "The market for" lemons": Quality uncertainty and the market mechanism." The quarterly journal of economics (1970): 488-500.}. Each of these topics required measurements of various types and researchers had to determine the best way to do that. As you might have guessed, researchers will measure just about anything that they have an interest in investigating. 

In 1964, philosopher Abraham Kaplan wrote what has since become a classic work in research methodology, \textit{The Conduct of Inquiry}\footnote{Kaplan, Abraham. The conduct of inquiry. Transaction Publishers, 1973.}. In his text, Kaplan describes different categories of things that behavioral scientists observe. One of those categories, which Kaplan called ``observational terms,'' is probably the simplest to measure, and are the sorts of things that can be seen with the naked eye simply by looking at them. They are terms that ``lend themselves to easy and confident verification.'' If, for example, researchers wanted to know how the conditions of playgrounds differ across different neighborhoods, they could directly observe the variety, amount, and condition of equipment at various playgrounds.

Indirect observables, on the other hand, are less straightforward to assess. They are ``terms whose application calls for relatively more subtle, complex, or indirect observations, in which inferences play an acknowledged part. Such inferences concern presumed connections, usually causal, between what is directly observed and what the term signifies.'' If researchers conducted a study for which they wished to know a person's income, they could simply ask in an interview or a survey. Thus, they would have observed income, even if it was only observed indirectly. Birthplace might be another indirect observable. Researchers can ask study participants where they were born, but chances are good that they will not directly observe any of those people being born in the locations they report.

Sometimes the measures that we are interested in are more complex and more abstract than observational terms or indirect observables. Think about concepts like ethnocentrism. That has to do with the way a person judges another person's culture, but measuring it would be very difficult. In the same way, a concept like  ``bureaucracy'' would be very difficult to measure. In both cases, ethnocentrism and bureaucracy, the theoretical notions represent ideas whose meaning is known but the measurement of the concept may be very challenging. Kaplan referred to these more abstract things as constructs. Constructs are ``not observational either directly or indirectly'' but they can be defined based on observables.

\subsection{How Do Researchers Measure?}

Measurement in business research is a process. It occurs at multiple stages of a research project: in the planning stages, in the data collection stage, and sometimes even in the analysis stage. 

As an example, imagine that the research question is: How do new college students cope with the adjustment to college? The first problem is to define ``cope'' in such a way that it can be measured. After that, the data collection phase can be designed to measure whatever ``cope'' means. After the data are collected then the analysis begins. Perhaps during the analysis phase an unexpected facet of coping is discovered and that may mean that the measures taken would need to be revisited to allow for that facet. Once the analysis is complete then there are certain decisions concerning the report. Perhaps one method of coping is determined to be more effective than others so the report may contain a recommendation that future research be conducted that measures just that one method of coping. The point is that measurement considerations are important throughout the research project.

The measurement process could also involve multiple stages. Starting with identifying and defining key terms to determining how to observe and measure them to assessing the quality of the measurements, there are multiple steps involved in the measurement process. An additional step in the measurement process involves deciding what type of data\marginpar{Data types are discussed on page \pageref{ch05:data}.} will be collected and an appropriate analysis process for those particular types of data elements. 

\section{Conceptualization}

This section explores one of the first steps in the measurement process, conceptualization, which is defining the terms as clearly as possible. Keep in mind that terms mean only what the researcher determines, nothing more and nothing less.

A \textit{concept} is the notion or image that is conjured up when the researcher thinks of some cluster of related observations or ideas. For example, masculinity is a concept. A researcher thinking about that concept may imagine some set of behaviors and perhaps even a particular style of self presentation. Of course, not everyone will conjure up that same set of ideas or images: in fact, there are many possible ways to define the term. While some definitions may be more common or have more support than others, there is not one true, always-correct-in-all-settings definition for ``masculine'' and that definition may well change over time, from culture to culture, and even from individual to individual \footnote{For those interested, the concept of masculinity is explored in Mosse, George L. The image of man: The creation of modern masculinity. Oxford University Press, 1998.}. This is why defining our concepts is so important.

It may seem unreasonable for a researcher to define a term for which there is no single, correct definition. Unfortunately, this will be a problem for most concepts measured in a business or marketing study. William Clinton, the 42\textsuperscript{d} President of the United States, famously stated ``It depends upon what the meaning of the word 'is' is.''\footnote{This was widely reported in the press and can be easily found on-line, including YouTube videos of him making that statement.} Without understanding how a researcher has defined the key concepts it would be impossible to understand the importance of the findings.

Defining concepts is an early part of the process of measurement called conceptualization, which involves writing out clear, concise definitions for key concepts. Brainstorming may help to conceptualize a topic, but it would also make sense to consult existing research and theory to see if other scholars have already defined the concepts of interest. This does not necessarily mean that their definitions are correct, but understanding how concepts have been defined in the past will help with a current project. Conceptualization is not as simple as merely applying a definition from a dictionary, it requires careful consideration and evaluating alternative concepts.

One important decision while conceptualizing constructs is specifying whether they are unidimensional or multidimensional. Unidimensional constructs are those that are expected to have a single underlying dimension and can be measured using a single measure or test. Examples include simple constructs such as a person's weight, wind speed, and even complex constructs like self-esteem (if self-esteem is conceptualized as consisting of a single dimension, which of course, may be unrealistic). Multidimensional constructs consist of two or more underlying dimensions. For instance, if a person's academic aptitude is conceptualized as consisting of two dimensions – mathematical and verbal ability – then academic aptitude is a multidimensional construct. Each of the underlying dimensions in this case must be measured separately by using different tests for mathematical and verbal ability, and then combine the two scores, possibly in a weighted manner, to create an overall value for the academic aptitude construct.

Before moving on to the next steps in the measurement process, it would be wise to consider one of the dangers associated with conceptualization. While it is important to consult prior scholarly definitions of key concepts, it would be wrong to assume that those definitions are any more real than whatever current definitions are generated by the researcher. It would also be wrong to assume that just because definitions exist for some concept that the concept itself exists beyond some abstract idea. This idea, assuming that abstract concepts exist in some concrete way is known as reification.

To better understand reification, take a moment to think about the concept of ``family.'' This concept is central to sociological thinking, but it is an abstract term. If researchers were interested in studying this concept, they would consult prior research to understand how the term has been conceptualized by others. But they should also question past conceptualizations. Today's conceptualization of ``family'' would be very different from one that was used $ 100 $ years ago. The point is that terms mean nothing more and nothing less than whatever definition is assigned by the researcher. Sure, it makes sense to come to some agreement about what various concepts mean. Without that agreement, it would be difficult to navigate through everyday living. But at the same time, it is important to remember that a society has assigned those definitions and that they are no more real than any other, alternative definition a researcher might choose to assign.

\section{Operationalization}

Once a theoretical construct is defined, indicators for measuring the construct are defined in a process called operationalization. For instance, if an unobservable theoretical construct such as socioeconomic status is defined as the level of family income then it can be operationalized using an indicator that asks respondents the question: what is your annual family income? Given the high level of subjectivity and imprecision inherent in social science constructs, most (except a few demographic constructs such as age, gender, education, and income) are measured using multiple indicators.
%TODO I have a nice graphic in my ch05 LO work, on page 4 of 12.
Indicators operate at the empirical level in contrast to constructs, which are conceptualized at the theoretical level. The combination of indicators at the empirical level representing a given construct is called a variable, and those may be independent, dependent, mediating, or moderating, depending on how they are employed in a research study. Also each indicator may have several attributes (or levels) and each attribute represent a value. For instance, a ``gender'' variable may have two attributes: male or female. Likewise, a customer satisfaction scale may be constructed to represent five attributes: ``strongly dissatisfied,'' ``somewhat dissatisfied,'' ``neutral,'' ``somewhat satisfied'' and ``strongly satisfied.'' Values of attributes may be quantitative (numeric) or qualitative (textual). Quantitative data can be analyzed using quantitative data analysis techniques, such as regression or structural equation modeling, while qualitative data require qualitative data analysis techniques, such as coding. Note that many variables in business research are qualitative, even when represented in a quantitative manner. For instance, a customer satisfaction indicator with five attributes: strongly dissatisfied, somewhat dissatisfied, neutral, somewhat satisfied, and strongly satisfied, can assign numbers $ 1-5 $ respectively for these five attributes, so that we can use sophisticated statistical tools for quantitative data analysis. However, note that the numbers are only labels associated with respondents' personal evaluation of their own satisfaction, and the underlying variable (satisfaction) is still qualitative even though we represented it in a quantitative manner.

Indicators may be reflective or formative. A reflective indicator is a measure that ``reflects'' an underlying construct. For example, if religiosity is defined as a construct that measures how religious a person is, then attending religious services may be a reflective indicator of religiosity. A formative indicator is a measure that ``forms'' or contributes to an underlying construct. Such indicators may represent different dimensions of the construct of interest. For instance, if religiosity is defined as composing of a belief dimension, a devotional dimension, and a ritual dimension, then indicators chosen to measure each of these different dimensions will be considered formative indicators. Unidimensional constructs are measured using reflective indicators (even though multiple reflective indicators may be used for measuring abstruse constructs such as self-esteem), while multidimensional constructs are measured as a formative combination of the multiple dimensions, even though each of the underlying dimensions may be measured using one or more reflective indicators.

It is important to keep in mind that the process of coming up with indicators cannot be arbitrary or casual. One way to avoid taking an overly casual approach in identifying indicators is to turn to prior theoretical and empirical work. Theories will point to relevant concepts and possible indicators while empirical work will detail specific examples of how key concepts have been measured in the past. One final important detail to think about when deciding on indicators is the strategy you will use for data collection. A survey implies one way of measuring concepts while field research implies a very different way. The data-collection strategy employed will play a major role in shaping how concepts are operationalized.

\section{Measurement Quality}

The previous section examined some of the difficulties with measuring constructs. What makes the task more challenging is that sometimes these constructs are imaginary concepts (i.e., they don’t exist in reality), and multi-dimensional (in which case, we have the added problem of identifying their constituent dimensions). Hence, it is not adequate just to measure constructs using any scale, the scales must be tested to ensure that: 

\begin{enumerate}
	\item they measure the construct consistently and precisely (i.e., the scales are ``reliable'') and 

	\item they actually measure the construct being investigated (i.e., the scales are ``valid''). 
\end{enumerate}

Reliability and validity are the yardsticks against which the adequacy and accuracy of our measurement procedures are evaluated in scientific research.
%TODO I have a ``4 Targets'' image for this section in my LO Ch05, page 5.
A measure can be reliable but not valid if it is measuring something very consistently but is measuring the wrong construct. Likewise, a measure can be valid but not reliable if it is measuring the right construct but not doing so in a consistent manner. Using the analogy of a shooting target, as shown in Figure xx, a measure that is both reliable and valid is like a group that is clustered near the center of the target. A measure that is reliable but not valid is like a group that is clustered but off-center. A measure that is valid but not reliable is a group that is centered but widely scattered. Finally, a measure that is neither reliable nor valid is like a group that is scattered and off-center. 

\subsection{Reliability}

Reliability ``...is the extent to which measurements are repeatable – when different persons perform the measurements, on different occasions, under different conditions, with supposedly alternative instruments which measure the same thing.\footnote{Drost, Ellen A. "Validity and reliability in social science research." Education Research and perspectives 38.1 (2011): 105.}''

Any score obtained by a measuring instrument (the observed score) is composed of both the ``true'' score, which is the score that a person would have received if the measurement were perfectly accurate, and the ``error'' in the measurement process. Imagine a simple example, a bathroom scale. If a person's true weight were $ 150 $ pounds then, ideally, the scale would read $ 150 $ every time that person stepped on the scale. The scale's reliability is the consistency of its output from one day to the next. If a person stepped on the scale one day and it read $ 160 $ but the next day it read $ 140 $ then the scale would not be a reliable instrument.

There are two types of reliability errors that researchers need to understand. First is systematic error, one that is caused by the system and is predictable. For example, the if the bathroom scale mentioned above constantly read five pounds heavy that would be an error, but it would be one that is consistent and could be corrected in the research data. That is an example of a systematic error. The second type of error is a random error. If the bathroom scale were accurate but the person reading it one day read $ 151 $ and the next as $ 149 $ then that would be a random error. Random errors cannot be corrected but tend to cancel out due to the random nature of the error (sometimes the reading will be a bit high and other times low), especially if there are many data points.

Unreliable measurements in business research could be for several reasons. One is the researcher's subjectivity. For example, if employee morale in a firm is being measured by watching whether the employees smile at each other, whether they make jokes, and so forth, then different observers may infer different measures of morale if they are watching the employees on a very busy day (when they have no time to joke or chat) or a light day (when they are more jovial or chatty). Two observers may also infer different levels of morale on the same day, depending on what they view as a joke and what is not. ``Observation'' is a qualitative measurement technique. 

Sometimes, reliability may be improved by using quantitative measures. Counting the number of grievances filed over one month as a measure of (the inverse of) morale. Of course, grievances may or may not be a valid measure of morale, but it is less subject to human subjectivity, and therefore more reliable. 

A second source of unreliable observation is asking imprecise or ambiguous questions. For instance, if people are asked to report their salary some may state a monthly salary, some an annual salary, and some even an hourly wage. Thus, the resulting observations will be divergent and unreliable. 

A third source of unreliability is asking questions about issues that respondents are not very familiar with or care about, such as asking an American college graduate about Canada's relationship with Slovenia or asking a Chief Executive Officer to rate the effectiveness of his company's technology strategy (which was likely delegated to a technology executive).

To improve reliability, start by replacing subjective data collection techniques (observation) with those that are more objective (questionnaire), ask respondents only questions that they may know or care about, avoid ambiguous items (e.g. clearly indicate annual salary), and simplify the wording in indicators. While these strategies can improve the reliability of measurements, instruments must still be tested for reliability using techniques like the following.

\begin{itemize}
	\item \textbf{Inter-rater reliability}. Inter-rater reliability, also called inter-observer reliability, is a measure of consistency between two or more independent raters (observers) of the same construct. Usually, this is assessed in a pilot study and can be done in two ways, depending on the level of measurement being used. If the measure is categorical, a set of all categories is defined, raters check off which category each observation falls in, and the percentage of agreement between the raters is used as an estimate of inter-rater reliability. For instance, if there are two raters rating $ 100 $ observations into one of three possible categories, and their ratings match for $ 75\% $ of the observations, then inter-rater reliability is $ 0.75 $. If the measure is interval or ratio scaled (e.g., classroom activity is being measured once every five minutes by two raters on one to seven scale), then a simple correlation between measures from the two raters can also serve as an estimate of inter-rater reliability.

	\item \textbf{Test-retest reliability}. Test-retest reliability is a measure of consistency between two measurements (tests) of the same construct administered to the same sample at two different points in time. If the observations have not changed substantially between the two tests, then the measure is reliable. The correlation in observations between the two tests is an estimate of test-retest reliability. Note here that the time interval between the two tests is critical. Generally, the longer is the time gap, the greater is the chance that the two observations may change during this time (due to random error), and the lower will be the test-retest reliability.

	\item \textbf{Split-half reliability}. Split-half reliability is a measure of consistency between two halves of a construct measure. For instance, if you have a ten-item measure of a given construct, randomly split those ten items into two sets of five (unequal halves are allowed if the total number of items is odd), and administer the entire instrument to a sample of respondents. Then, calculate the total score for each half for each respondent, and the correlation between the total scores in each half is a measure of split-half reliability. The longer the instrument, the more likely it is that the two halves of the measure will be similar (since random errors are minimized as more items are added), and hence, this technique tends to systematically overestimate the reliability of longer instruments.

	\item \textbf{Internal consistency reliability}. Internal consistency reliability is a measure of consistency between different items of the same construct. If a multiple-item construct measure is administered to respondents, the extent to which respondents rate those items in a similar manner is a reflection of internal consistency. This reliability can be estimated in terms of average inter-item correlation, average item-to-total correlation, or more commonly, \textit{Cronbach’s alpha}. 

\end{itemize}

\subsection{Validity}

Validity is concerned with the meaningfulness of research results. In brief, does the research actually measure what it was purported to measure? For example, does the Scholastic Aptitude Test (SAT) actually predict the likelihood of a high school student successfully completing college?\footnote{Drost, Ellen A. ``Validity and reliability in social science research.'' Education Research and perspectives 38.1 (2011): 105.} There are numerous types of validity found in the literature, but they generally form two large groups: Measurement Validity (the measurement should accurately reflect the construct) and Hypothesis Validity (the hypotheses should accurately reflect the construct).

\subsubsection{Measurement Validity}

The theoretical assessment of validity focuses on how well a construct is translated into an operational measure. This type of validity is called \textit{translational validity} and consists of two subtypes: face and content validity. Translational validity is typically assessed using a panel of expert judges who rate each item (indicator) on how well it fits the conceptual definition of that construct along with a qualitative technique called \textit{Q-method}\footnote{For an explanation of Q-method, see Shinebourne, Pnina. "Using Q method in qualitative research." International Journal of Qualitative Methods 8.1 (2009): 93-97.}.

The empirical assessment of validity examines how well a given measure relates to one or more external criterion, based on empirical observations. This type of validity is called \textit{criterion-related validity}, which includes four sub-types: convergent, discriminant, concurrent, and predictive validity. While translation validity examines whether a measure is a good reflection of its underlying construct, criterion-related validity examines whether a given measure behaves the way it should, given the theory of that construct. Criterion-referenced assessment is based on quantitative analysis of observed data using statistical techniques such as correlational analysis, factor analysis, and so forth. The distinction between theoretical and empirical assessment of validity is illustrated in Figure xxx. However, both approaches are needed to adequately ensure the validity of measures in business research.

\begin{description}
	\item[Face validity]. Face validity refers to whether an indicator seems to be a reasonable measure of its underlying construct ``on its face.'' For instance, the frequency of one's attendance at religious services seems to make sense as an indication of a person's religiosity without a lot of explanation. Hence this indicator has face validity. However, if we were to suggest how many books were checked out of an office library as a measure of employee morale, then such a measure would probably lack face validity because it does not seem to make much sense. Interestingly, some of the popular measures used in organizational research appears to lack face validity. For instance, absorptive capacity of an organization (how much new knowledge can it assimilate for improving organizational processes) has often been measured as research and development intensity (i.e., R\&D expenses divided by gross revenues)! If your research includes constructs that are highly abstract or constructs that are hard to conceptually separate from each other (e.g., compassion and empathy), it may be worthwhile to consider using a panel of experts to evaluate the face validity of your construct measures.

%TODO Note: nice graphic here

	\item[Content validity]. Content validity is an assessment of how well a set of scale items matches with the relevant content domain of the construct that it is trying to measure. For instance, if you want to measure the construct ``satisfaction with restaurant service,'' and you define the content domain of restaurant service as including the quality of food, courtesy of wait staff, duration of wait, and the overall ambiance of the restaurant (i.e., whether it is noisy, smoky, etc.), then for adequate content validity, this construct should be measured using indicators that examine the extent to which a restaurant patron is satisfied with the quality of food, courtesy of wait staff, the length of wait, and the restaurant's ambiance. Of course, this approach requires a detailed description of the entire content domain of a construct, which may be difficult for complex constructs such as self-esteem or intelligence. Hence, it may not be always possible to adequately assess content validity. As with face validity, an expert panel of judges may be employed to examine content validity of constructs.

	\item[Convergent validity]. Convergent validity refers to the closeness with which a measure relates to (or converges on) the construct that it is purported to measure, and discriminant validity refers to the degree to which a measure does not measure (or discriminates from) other constructs that it is not supposed to measure. Usually, convergent validity and discriminant validity are assessed jointly for a set of related constructs. For instance, if you expect that an organization's knowledge is related to its performance, how can you assure that your measure of organizational knowledge is indeed measuring organizational knowledge (for convergent validity) and not organizational performance (for discriminant validity)? Convergent validity can be established by comparing the observed values of one indicator of one construct with that of other indicators of the same construct and demonstrating similarity (or high correlation) between values of these indicators. Discriminant validity is established by demonstrating that indicators of one construct are dissimilar from (i.e., have low correlation with) other constructs. In the above example, if we have a three-item measure of organizational knowledge and three more items for organizational performance, based on observed sample data, we can compute bivariate correlations between each pair of knowledge and performance items. If this correlation matrix shows high correlations within items of the organizational knowledge and organizational performance constructs, but low correlations between items of these constructs, then we have simultaneously demonstrated convergent and discriminant validity.

	An alternative and more common statistical method used to demonstrate convergent and discriminant validity is exploratory factor analysis. This is a data reduction technique which aggregates a given set of items to a smaller set of factors based on the bivariate correlation structure discussed above using a statistical technique called principal components analysis. These factors should ideally correspond to the underling theoretical constructs that we are trying to measure. The general norm for factor extraction is that each extracted factor should have an eigenvalue greater than $ 1.0 $. The extracted factors can then be rotated using orthogonal or oblique rotation techniques, depending on whether the underlying constructs are expected to be relatively uncorrelated or correlated, to generate factor weights that can be used to aggregate the individual items of each construct into a composite measure. For adequate convergent validity, it is expected that items belonging to a common construct should exhibit factor loadings of $ 0.60 $ or higher on a single factor (called same-factor loadings), while for discriminant validity, these items should have factor loadings of $ 0.30 $ or less on all other factors (cross-factor loadings), as shown in rotated factor matrix example in Table xxx. A more sophisticated technique for evaluating convergent and discriminant validity is the multi-trait multi-method (\textsc{MTMM}) approach. This technique requires measuring each construct (trait) using two or more different methods (e.g., survey and personal observation, or perhaps survey of two different respondent groups such as teachers and parents for evaluating academic quality). This is an onerous and relatively less popular approach, and is therefore not discussed here.

	\item[Criterion-related validity]. Criterion-related validity can also be assessed based on whether a given measure relates well with a current or future criterion, which are respectively called concurrent and predictive validity. Predictive validity is the degree to which a measure successfully predicts a future outcome that it is theoretically expected to predict. For instance, can standardized test scores (e.g., Scholastic Aptitude Test scores) correctly predict the academic success in college (e.g., as measured by college grade point average)? Assessing such validity requires creation of a ``nomological network'' showing how constructs are theoretically related to each other. Concurrent validity examines how well one measure relates to other concrete criterion that is presumed to occur simultaneously. For instance, do students' scores in a calculus class correlate well with their scores in a linear algebra class? These scores should be related concurrently because they are both tests of mathematics. Unlike convergent and discriminant validity, concurrent and predictive validity is frequently ignored in empirical business research.

\end{description}

\subsubsection{Hypothesis Validity}

In general, four types of hypothesis validity are referred to in the literature: 1) internal validity, 2) external validity, 3) construct validity, and 4) statistical conclusion validity.

\begin{description}
	\item[Internal validity]. Internal validity, also called causality, examines whether the observed change in a dependent variable is caused by a corresponding change in hypothesized independent variable and not by variables extraneous to the research context. Causality requires three conditions: 1) covariation of cause and effect (if cause happens then effect also happens and if cause does not happen effect does not happen), 2) temporal precedence (cause must precede effect in time), and 3) lack of plausible alternative explanation (or spurious correlation). Certain research designs, such as laboratory experiments, are strong in internal validity since researchers can manipulate the independent variable (cause) via a treatment and observe the effect (dependent variable) of that treatment after a certain point in time while controlling for the effects of extraneous variables. Other designs, such as field surveys, are poor in internal validity because researchers cannot manipulate the independent variable (cause) and because cause and effect are measured at the same point in time which defeats temporal precedence making it equally likely that the effect actually brought about the presumed cause rather than the reverse. 
	
	%There is a nice graphic of validity and the ``cone'' of types of validity, Anol p 45.
	
	\item[External Validity]. External validity, or generalizability, refers to whether the observed associations can be generalized from the sample to the population (population validity), or to entities outside the population (ecological validity). For instance, if results drawn from a sample of financial firms in the United States can be generalized to the population of all financial firms it would have strong population validity and if to other types of firms it would have strong ecological validity. Survey research, where data are sourced from a wide variety of individuals, firms, or other units of analysis, tends to have broader generalizability than laboratory experiments where artificially contrived treatments and strong control over extraneous variables render the findings less generalizable to real-life settings where treatments and extraneous variables cannot be controlled. \marginpar{Some researchers claim that increased external validity leads to decreased internal validity and vice-versa, but this is not always true. Some research designs, such as multiple case studies, have high degrees of both internal and external validities.}
	
	\item[Construct Validity]. Construct validity examines how well a given measurement scale is measuring the theoretical construct that it is designed to measure. One frequent problem with construct validity is simply defining the construct in such a way that it is measurable. As one example, ``property ownership'' is a construct of a market economy\footnote{This construct was discussed in Reich, Robert B. Saving capitalism: For the many, not the few. Vintage, 2016.}. That is, the fact that people can own property drives a local economy. But this construct relies on a number of external forces that cannot be controlled, such as local politics (a city's eminent domain can take a person's property) and the value of the property on the open market. Measuring the influence of property ownership on a local economy (the construct) would be very difficult since there are so many confounding variables. 
	
	\item[Statistical Conclusion]. Statistical conclusion validity examines the extent to which conclusions derived from a statistical procedure are valid. For example, it examines whether the right statistical method was used and whether the variables meet the assumptions of that statistical test (such as sample size or distributional requirements). 
	
\end{description}


\subsection{Improving Internal and External Validity}

The best research designs are those that can assure high levels of internal and external validity. Such designs would guard against spurious correlations, inspire greater faith in the hypotheses testing, and ensure that the results drawn from a small sample are generalizable to the population at large. The internal validity of research designs and can be improved using four methods: 1) manipulation, 2) elimination, 3) inclusion, and 4) randomization.

\begin{enumerate}
	\item Manipulation involves the researcher manipulating the independent variables in one or more ways (called ``treatments''), and compares the effects of the treatments against a control group where subjects do not receive the treatment. Treatments may include a new drug or different dosage of drug (for treating a medical condition), a new teaching style (for education), and so forth. This type of control can be achieved in experimental or quasi-experimental designs but not in non-experimental designs such as surveys. 
	
	\item Elimination relies on eliminating extraneous variables by holding them constant across treatments, such as by restricting the study to a single gender or a single socioeconomic status. 
	
	\item Inclusion is the process of separately estimating the effects of spurious variables on the dependent variable. As an example, consider the process of estimating the effect of gender on a marketing study. Inclusion techniques allow for greater generalizability of the study but also require substantially larger samples. 
	
	\item Randomization is aimed at canceling out the effects of extraneous variables through a process of random sampling. Two types of randomization are: 1) random selection, where a sample is selected randomly from a population, and 2) random assignment, where subjects selected in a non-random manner are randomly assigned to treatment groups. Randomization also improves external validity, allowing inferences drawn from the sample to be generalized to the population from which the sample is drawn; however, generalizability across populations is harder to ascertain since populations may differ on multiple dimensions and only a few of those dimensions can be controlled.
	
\end{enumerate}

\section{Summary}\label{ch06:summary}

\begin{itemize}
	\setlength{\itemsep}{0pt}
	\setlength{\parskip}{0pt}
	\setlength{\parsep}{0pt}
	
	\item In social science, our variables can be one of four different levels of measurement: nominal, ordinal, interval, or ratio.
	\item Indexes and typologies allow us to account for and simplify some of the complexities in our measures.
	
\end{itemize}
