%*****************************************
\chapter{Defining and Measuring Concepts}\label{ch05:measuring}
%*****************************************
%TODO Status: Bibligraphy and Glossary checked

\section{Measurement}

\begin{wrapfigure}{r}{0.42\textwidth}
	\label{05:fig01} 
	\centering
	\includegraphics[width=0.4\textwidth]{gfx/05-cake} 
\end{wrapfigure}

Measurement is important. People who have attempted to bake a cake from scratch without measuring the ingredients will find, no doubt, that measurement is the difference between a sweet desert and a disaster. Just like in baking, measurement is important to a researcher. Measurement means the process by which key facts, attributes, concepts, and other phenomena are described. At its core, measurement is about defining the research project's terms in a precise and measurable way. Of course, measurement in business research is not quite as simple as using some predetermined or universally agreed-on tool, such as a measuring cup, but there are some basic tenants on which most researchers agree when it comes to measurement.\blfootnote{Photo by lindsay Cotter on Unsplash}


\subsection{What Do Researchers Measure?}

The question of what business researchers measure can be answered by asking what business researchers study. Researchers study a wide variety of business and marketing concepts, like corporate culture\cite{denison1990corporate}, the price elasticity of gasoline\cite{hughes2006evidence}, employee turnover\cite{hom1995employee}, and automobile ``lemons''\cite{akerlof1978market}. Each of these topics required measurements of various types and researchers had to determine the best way to do that. As you might have guessed, researchers will measure just about anything that they have an interest in investigating. 

In 1964, philosopher Abraham Kaplan wrote what has since become a classic work in research methodology, \textit{The Conduct of Inquiry}\cite{kaplan2017conduct}. In his text, Kaplan describes different categories of things that behavioral scientists observe. One of those categories, which Kaplan called ``observational terms,'' is probably the simplest to measure, and are the sorts of things that can be seen with the naked eye simply by looking at them. They are terms that ``lend themselves to easy and confident verification.'' If, for example, researchers wanted to know how the conditions of playgrounds differ across different neighborhoods, they could directly observe the variety, amount, and condition of equipment at various playgrounds.

Indirect observables, on the other hand, are less straightforward to assess. They are ``terms whose application calls for relatively more subtle, complex, or indirect observations, in which inferences play an acknowledged part. Such inferences concern presumed connections, usually causal, between what is directly observed and what the term signifies.'' If researchers conducted a study for which they wished to know a person's income, they could simply ask in an interview or a survey. Thus, they would have observed income, even if it was only observed indirectly. Birthplace might be another indirect observable. Researchers can ask study participants where they were born, but chances are good that they will not directly observe any of those people being born in the locations they report.

Sometimes the measures that we are interested in are more complex and more abstract than observational terms or indirect observables. Think about concepts like ethnocentrism, the way a person judges another person's culture, and how measuring that concept would be very challenging. In the same way, a concept like  ``bureaucracy'' would be very difficult to measure. In both cases, ethnocentrism and bureaucracy, the theoretical notions represent ideas whose meaning is known but the measurement of the concept may be nearly impossible. Kaplan referred to these more abstract things as \glspl{construct}. Constructs are ``not observational either directly or indirectly'' but they can be defined based on observables.

\subsection{How Do Researchers Measure?}

Measurement in business research is a process. It occurs at multiple stages of a research project: in the planning stage, in the data collection stage, and sometimes even in the analysis stage. 

As an example, imagine that the research question is: How do new college students cope with the adjustment to college? The first problem is to define ``cope'' in such a way that it can be measured. After that, the data collection phase can be designed to measure whatever ``cope'' means. After the data are collected then the analysis begins. Perhaps during the analysis phase an unexpected facet of coping is discovered and that may mean that the measures taken would need to be revisited to allow for that facet. Once the analysis is complete then there are certain decisions concerning the report. Perhaps one method of coping is determined to be more effective than others so the report may contain a recommendation that future research be conducted that measures just that one method of coping. The point is that measurement considerations are important throughout the research project.

The measurement process could also involve multiple stages. Starting with identifying and defining key terms to determining how to observe and measure them to assessing the quality of the measurements, there are multiple steps involved in the measurement process. An additional step in the measurement process involves deciding what type of data\marginpar{Data types are discussed on page \pageref{ch05:data}.} will be collected and an appropriate analysis process for those particular types of data elements. 

\section{Conceptualization}

One of the first steps in the measurement process is conceptualization, which is defining the terms of the project as clearly as possible. Keep in mind that terms mean only what the researcher determines, nothing more and nothing less.

A \textit{concept} is the notion or image that is conjured up when the researcher thinks of some cluster of related observations or ideas. For example, masculinity is a concept. A researcher thinking about that concept may imagine some set of behaviors and perhaps even a particular style of self presentation. Of course, not everyone will conjure up that same set of ideas or images: in fact, there are many possible ways to define the term. While some definitions may be more common or have more support than others, there is not one true, always-correct-in-all-settings definition for ``masculine'' and that definition may well change over time, from culture to culture, and even from individual to individual, as explained by George Mosse\cite{george1996image}. This is why defining concepts is so important before any data gathering begins.

It may seem unreasonable for a researcher to define a term for which there is no single, correct definition. Unfortunately, this will be a problem for most concepts measured in a business or marketing study. William Clinton, the 42\textsuperscript{d} President of the United States, famously stated ``It depends upon what the meaning of the word 'is' is.''\footnote{This was widely reported in the press and can be easily found on-line, including YouTube videos of him making that statement.} Without understanding how a researcher has defined the key concepts it would be impossible to understand the importance of the findings.

Defining concepts is an early part of the process of measurement called conceptualization, which involves writing out clear, concise definitions for key concepts. Brainstorming may help to conceptualize a topic, but it would also make sense to consult existing research and theory to see if other scholars have already defined the concepts of interest. This does not necessarily mean that their definitions are correct, but understanding how concepts have been defined in the past will help with a current project. Conceptualization is not as simple as merely applying a definition from a dictionary, it requires careful consideration and evaluating alternative concepts.

One important decision while conceptualizing constructs is specifying whether they are unidimensional or multidimensional. Unidimensional constructs are those that are expected to have a single underlying dimension and can be measured using a single measure or test. Examples include simple constructs such as a person's weight, wind speed, and even complex constructs like self-esteem (if self-esteem is conceptualized as consisting of a single dimension, which of course, may be unrealistic). Multidimensional constructs consist of two or more underlying dimensions. For instance, if a person's academic aptitude is conceptualized as consisting of two dimension, mathematical and verbal ability, then academic aptitude is a multidimensional construct. Each of the underlying dimensions in this case must be measured separately by using different tests for mathematical and verbal ability, and then combine the two scores, possibly in a weighted manner, to create an overall value for the academic aptitude construct.

Before moving on to the next steps in the measurement process, it would be wise to consider one of the dangers associated with conceptualization. While it is important to consult prior scholarly definitions of key concepts, it would be wrong to assume that those definitions are any more real than whatever current definitions are generated by the researcher. It would also be wrong to assume that just because definitions exist for some concept that the concept itself exists beyond some abstract idea. This idea, assuming that abstract concepts exist in some concrete way is known as reification.

To better understand reification, take a moment to think about the concept of ``family.'' This concept is central to sociological thinking, but it is an abstract term. If researchers were interested in studying this concept, they would consult prior research to understand how the term has been conceptualized by others. But they should also question past conceptualizations. Today's conceptualization of ``family'' would be very different from one that was used a hundred years ago. The point is that terms mean nothing more and nothing less than whatever definition is assigned by the researcher. Sure, it makes sense to come to some agreement about what various concepts mean. Without that agreement, it would be difficult to navigate through everyday living. But at the same time, it is important to remember that a society has assigned those definitions and that they are no more real than any other, alternative definition a researcher might choose to assign.

\section{Operationalization}

Once a theoretical construct is defined, indicators for measuring the construct are defined in a process called operationalization. For instance, if an unobservable theoretical construct such as socioeconomic status is defined as the level of family income then it can be operationalized using an indicator that asks respondents the question: what is your annual family income? Given the high level of subjectivity and imprecision inherent in social science constructs, most (except a few demographic constructs such as age, gender, education, and income) are measured using multiple indicators.

\begin{figure}[H]
	\centering
	\includegraphics[width=\maxwidth{.95\linewidth}]{gfx/05-ConceptVsOper}
	\caption{Conceptualization Vs. Operationalization}
	\label{05:fig02}
\end{figure}

Indicators operate at the empirical level in contrast to constructs, which are conceptualized at the theoretical level. The combination of indicators at the empirical level representing a given construct is called a \gls{variable}, and those may be independent, dependent, mediating, or moderating, depending on how they are employed in a research study. Also each indicator may have several attributes (or levels) and each attribute represent a value. For instance, a ``gender'' variable may have two attributes: male or female. Likewise, a customer satisfaction scale may be constructed to represent five attributes: ``strongly dissatisfied,'' ``somewhat dissatisfied,'' ``neutral,'' ``somewhat satisfied'' and ``strongly satisfied.'' 

Variables may be quantitative (numeric) or qualitative (textual). Quantitative data can be analyzed using techniques like regression or structural equation modeling while qualitative data uses techniques like coding. Note that many variables in business research are qualitative, even when represented in a quantitative manner. For instance, a customer satisfaction indicator with five attributes: strongly dissatisfied, somewhat dissatisfied, neutral, somewhat satisfied, and strongly satisfied, can assign the numbers $ 1-5 $ respectively for these five attributes, so that we can use sophisticated statistical tools for quantitative data analysis. However, note that the numbers are only labels associated with respondents' personal evaluation of their own satisfaction, and the underlying variable (satisfaction) is still qualitative even though it is represented numerically.

Indicators may be reflective or formative. A reflective indicator is a measure that ``reflects'' an underlying construct. For example, if religiosity is defined as a construct that measures how religious a person is, then attending religious services may be a reflective indicator of religiosity. A formative indicator is a measure that ``forms'' or contributes to an underlying construct. Such indicators may represent different dimensions of the construct of interest. For instance, if religiosity is defined as composed of a belief dimension, a devotional dimension, and a ritual dimension, then indicators chosen to measure each of these different dimensions will be considered formative indicators. Unidimensional constructs are measured using reflective indicators (even though multiple reflective indicators may be used for measuring abstruse constructs such as self-esteem), while multidimensional constructs are measured as a formative combination of the multiple dimensions, even though each of the underlying dimensions may be measured using one or more reflective indicators.

It is important to keep in mind that the process of coming up with indicators cannot be arbitrary or casual. One way to avoid taking an overly casual approach in identifying indicators is to turn to prior theoretical and empirical work. Theories will point to relevant concepts and possible indicators while empirical work will detail specific examples of how key concepts have been measured in the past. One final important detail to think about when deciding on indicators is the strategy you will use for data collection. A survey implies one way of measuring concepts while field research implies a very different way. The data-collection strategy employed will play a major role in shaping how concepts are operationalized.

\section{Measurement Quality}

The previous section examined some of the difficulties with measuring constructs. What makes the task more challenging is that sometimes these constructs are imaginary concepts (i.e., they don’t exist in reality), and multi-dimensional (in which case, we have the added problem of identifying their constituent dimensions). Hence, it is not adequate just to measure constructs using any scale, the scales must be tested to ensure that: 

\begin{enumerate}
	\item they measure the construct consistently and precisely (i.e., the scales are ``reliable'') and 

	\item they actually measure the construct being investigated (i.e., the scales are ``valid''). 
\end{enumerate}

Reliability, the consistency of a measure, and validity, the efficacy of a measure, are the two yardsticks against which the accuracy of measurements are evaluated in scientific research. A measure can be reliable but not valid if it is measuring consistently but it is the wrong construct. Likewise, a measure can be valid but not reliable if it is measuring the right construct but not doing so in a consistent manner. Using the analogy of a shooting target, as shown in Figure \ref{05:fig03}, a measure that is both reliable and valid is like a group that is tightly clustered near the center of the target. A measure that is reliable but not valid is like a group that is tightly clustered but off-center. A measure that is valid but not reliable is a group that is widely scattered but centered. Finally, a measure that is neither reliable nor valid is like a group that is widely scattered and off-center. 

\begin{figure}[H]
	\centering
	\includegraphics[width=\maxwidth{.95\linewidth}]{gfx/05-Targets}
	\caption{Reliability Analogy}
	\label{05:fig03}
\end{figure}

\subsection{Reliability}

\Gls{reliability} ``...is the extent to which measurements are repeatable – when different persons perform the measurements, on different occasions, under different conditions, with supposedly alternative instruments which measure the same thing.''\cite{drost2011validity}

Any score obtained by a measuring instrument (the observed score) is composed of both the ``true'' score, which is the score that a person would have received if the measurement were perfectly accurate, and the ``error'' in the measurement process. Imagine a simple example, a bathroom scale. If a person's true weight were $ 150 $ pounds then, ideally, the scale would read $ 150 $ every time that person stepped on the scale. The scale's reliability is the consistency of its output from one day to the next. If a person stepped on the scale one day and it read $ 160 $ but the next day it read $ 140 $ then the scale would not be a reliable instrument.

There are two types of reliability errors that researchers need to understand. First is systematic error, one that is caused by the system and is predictable. For example, the if the bathroom scale mentioned above constantly read five pounds heavy that would be an error, but it would be one that is consistent and could be corrected in the research data. That is an example of a systematic error. The second type of error is a random error. If the bathroom scale were accurate but the person reading it one day read $ 151 $ and the next as $ 149 $ then that would be a random error. Random errors cannot be corrected but tend to cancel out due to the random nature of the error (sometimes the reading will be a bit high and other times low), especially if there are many data points.

Unreliable measurements in business research could be for several reasons. One is the researcher's subjectivity. For example, if employee morale in a firm is being measured by watching whether the employees smile at each other, whether they make jokes, and so forth, then different observers may infer different measures of morale if they are watching the employees on a very busy day (when they have no time to joke or chat) or a light day (when they are more jovial or chatty). Two observers may also infer different levels of morale on the same day, depending on what they view as a joke and what is not. ``Observation'' is a qualitative measurement technique. 

Sometimes, reliability may be improved by using quantitative measures. Counting the number of grievances filed over one month as a measure of (the inverse of) morale. Of course, grievances may or may not be a valid measure of morale, but it is less subject to human subjectivity, and therefore more reliable. 

A second source of unreliable observation is asking imprecise or ambiguous questions. For instance, if people are asked to report their salary some may state a monthly salary, some an annual salary, and some even an hourly wage. Thus, the resulting observations will be divergent and unreliable. 

A third source of unreliability is asking questions about issues that respondents are not very familiar with or care about, such as asking an American college graduate about Canada's relationship with Slovenia or asking a Chief Executive Officer to rate the effectiveness of his company's technology strategy (which was likely delegated to a technology executive).

To improve reliability, start by replacing subjective data collection techniques (observation) with those that are more objective (questionnaire), ask respondents only questions that they may know or care about, avoid ambiguous items (e.g. clearly indicate annual salary), and simplify the wording in indicators. While these strategies can improve the reliability of measurements, instruments must still be tested for reliability using techniques like the following.

\begin{itemize}
	\item \textbf{Inter-rater reliability}. Inter-rater reliability, also called inter-observer reliability, is a measure of consistency between two or more independent raters (observers) of the same construct. Usually, this is assessed in a pilot study and can be done in two ways, depending on the level of measurement being used. If the measure is categorical, a set of all categories is defined, raters check off which category each observation falls in, and the percentage of agreement between the raters is used as an estimate of inter-rater reliability. For instance, if there are two raters rating $ 100 $ observations into one of three possible categories, and their ratings match for $ 75\% $ of the observations, then inter-rater reliability is $ 0.75 $. If the measure is interval or ratio scaled (e.g., classroom activity is being measured once every five minutes by two raters on one to seven scale), then a simple correlation between measures from the two raters can also serve as an estimate of inter-rater reliability.

	\item \textbf{Test-retest reliability}. Test-retest reliability is a measure of consistency between two measurements (tests) of the same construct administered to the same sample at two different points in time. If the observations have not changed substantially between the two tests, then the measure is reliable. The correlation in observations between the two tests is an estimate of test-retest reliability. Note here that the time interval between the two tests is critical. Generally, the longer is the time gap, the greater is the chance that the two observations may change during this time (due to random error), and the lower will be the test-retest reliability.

	\item \textbf{Split-half reliability}. Split-half reliability is a measure of consistency between two halves of a construct measure. For instance, if you have a ten-item measure of a given construct, randomly split those ten items into two sets of five (unequal halves are allowed if the total number of items is odd), and administer the entire instrument to a sample of respondents. Then, calculate the total score for each half for each respondent, and the correlation between the total scores in each half is a measure of split-half reliability. The longer the instrument, the more likely it is that the two halves of the measure will be similar (since random errors are minimized as more items are added), and hence, this technique tends to systematically overestimate the reliability of longer instruments.

	\item \textbf{Internal consistency reliability}. Internal consistency reliability is a measure of consistency between different items of the same construct. If a multiple-item construct measure is administered to respondents, the extent to which respondents rate those items in a similar manner is a reflection of internal consistency. This reliability can be estimated in terms of average inter-item correlation, average item-to-total correlation, or more commonly, \textit{Cronbach’s alpha}. 

\end{itemize}

\subsection{Validity}

\Gls{validity} is concerned with the meaningfulness of research results. In brief, does the research actually measure what it was purported to measure? For example, does the Scholastic Aptitude Test (SAT) actually predict the likelihood of a high school student successfully completing college?\cite{drost2011validity} There are numerous types of validity found in the literature, but they generally form two large groups: Measurement Validity (the measurement should accurately reflect the construct) and Hypothesis Validity (the hypotheses should accurately reflect the construct).

\subsubsection{Measurement Validity}

The \textit{theoretical} assessment of validity focuses on how well an abstract construct is translated into an operational measure, which is called \gls{translationalvalidity}, and divided into two sub-types: face and content validity. Translational validity is typically assessed using a panel of expert judges who rate each item (indicator) on how well it fits the conceptual definition of that construct along with a qualitative technique called \textit{Q-method}, as explained by Pnina Shinebourne\cite{shinebourne2009using}.

The \textit{empirical} assessment of validity examines how well a given measure relates to one or more external criterion, based on empirical observations. This type of validity is called \gls{criterionvalidity}, which is divided into four sub-types: convergent, discriminant, concurrent, and predictive. While translation validity examines whether a measure is a good reflection of its underlying construct, criterion-related validity examines whether a given measure behaves the way it should, given the theory of that construct. The distinction between theoretical and empirical assessment of validity is illustrated in Figure \ref{05:fig04}. However, both approaches are needed to adequately ensure the validity of measures in business research.

\begin{figure}[H]
	\centering
	\includegraphics[width=\maxwidth{.95\linewidth}]{gfx/05-AssessValidity}
	\caption{Assessing Theoretical and Empirical Validity}
	\label{05:fig04}
\end{figure}

\paragraph{Translational Validity}

\begin{description}
	\item[\Gls{facevalidity}] refers to whether an indicator seems to be a reasonable measure of its underlying construct ``on its face.'' For instance, the frequency of attendance at religious services seems to make sense as an indication of a person's religiosity without a lot of explanation. Hence this indicator has face validity. However, if we were to suggest how many books were checked out of an office library as a measure of employee morale, then such a measure would probably lack face validity because it does not seem to make much sense. Interestingly, some of the popular measures used in organizational research appear to lack face validity. For instance, absorptive capacity of an organization (how much new knowledge can it assimilate for improving organizational processes) has often been measured by research and development intensity (\ie, R\&D expenses divided by gross revenues). Research that includes constructs that are highly abstract or are hard to conceptually separate from each other (\eg, compassion and empathy), it may be worthwhile to use a panel of experts to evaluate the face validity of the measures.

	\item[\Gls{contentvalidity}] is an assessment of how well a measure matches the content domain of the construct being measured. For instance, to measure the construct ``satisfaction with restaurant service,'' then the content domain should include the quality of food, courtesy of wait staff, duration of wait, and the overall ambiance of the restaurant (i.e., whether it is noisy, smoky, etc.). The project should measure the extent to which a restaurant patron is satisfied with the quality of food, courtesy of wait staff, the length of wait, and the restaurant's ambiance. Of course, this approach assumes the researcher can create a detailed description of the content domain, which may be difficult for complex constructs such as self-esteem or intelligence. As with face validity, an expert panel of judges may be employed to examine content validity of constructs.
\end{description}

\paragraph{Criterion-Related Validity}

\begin{description}

	\item[\Gls{convergentvalidity}] refers to the closeness with which a measure, or group of measures, relates to (or converges on) the construct that it is purported to measure. Convergent validity can be established by comparing the observed values of one indicator with those of other indicators to attempt to find high correlation between those indicators. Compare this to discriminant validity.
	
	\item[\gls{discriminantvalidity}] refers to the degree to which a measure does not measure (or discriminates from) other constructs that it is not supposed to measure. Usually, convergent validity and discriminant validity are assessed jointly for a set of related constructs. For example, if an organization's knowledge is related to its performance then a measure of organizational knowledge must actually measure organizational knowledge (convergent validity) and not organizational performance (discriminant validity). Discriminant validity is established by demonstrating that indicators of one construct are dissimilar from (i.e., have low correlation with) other constructs.

	\item[\Gls{concurrentvalidity}] examines how well a measure of one outcome relates to another outcome that is presumed to occur simultaneously. For instance, do students' scores in a calculus class correlate well with their scores in a linear algebra class? Since both are mathematics classes it would be presumed that there is high concurrent validity between scores in those classes. 
	
	\item[\Gls{predictivevalidity}] is the degree to which a measure successfully predicts a future outcome. For example, a standardized test score (\eg, \textit{Scholastic Aptitude Test}) can be used to predict a student's academic success in college. Concurrent and predictive validity are not often considered in empirical business research.

\end{description}

\subsubsection{Hypothesis Validity}

In general, four types of hypothesis validity are referred to in the literature.

\begin{description}
	\item[\Gls{internalvalidity}] examines whether the observed change in a dependent variable is caused by a corresponding change in hypothesized independent variable and not by variables extraneous to the research context. This is sometimes called ``causality'' and it requires three conditions: 
	
	
	\begin{enumerate}
		\item covariation of cause and effect (if cause happens then effect also happens and if cause does not happen effect does not happen)
		\item temporal precedence (cause must precede effect in time)
		\item lack of plausible alternative explanation (or spurious correlation). 
	\end{enumerate}
	
	Certain research designs, such as laboratory experiments, are strong in internal validity since researchers can manipulate the independent variable (cause) via a treatment and observe the effect (dependent variable) of that treatment after a certain point in time while controlling for the effects of extraneous variables. Other designs, such as field surveys, are poor in internal validity because researchers cannot manipulate the independent variable (cause) and because cause and effect are measured at the same point in time which defeats temporal precedence making it equally likely that the effect actually brought about the presumed cause rather than the reverse. 
	
	%There is a nice graphic of validity and the ``cone'' of types of validity, Anol p 45.
	
	\item[\Gls{externalvalidity}] refers to whether the observed associations can be generalized from the sample to the population (population validity), or to entities outside the population (ecological validity). For example, if results drawn from a sample of financial firms in the United States can be generalized to the population of all financial firms it would have strong population validity and if to other types of firms it would have strong ecological validity. Survey research, where data are sourced from a wide variety of individuals, firms, or other units of analysis, tends to have broader generalizability than laboratory experiments where artificially contrived treatments and strong control over extraneous variables render the findings less generalizable to real-life settings where treatments and extraneous variables cannot be controlled. \marginpar{Some researchers claim that increased external validity leads to decreased internal validity and vice-versa, but this is not always true. Some research designs, such as multiple case studies, have high degrees of both internal and external validities.}
	
	\item[\Gls{constructvalidity}] examines how well a given measurement scale is measuring the theoretical construct that it is designed to measure. One frequent problem with construct validity is simply defining the construct in such a way that it is measurable. As one example, ``property ownership'' is a construct of a market economy explained by Robert Reich\cite{reich2016saving}. That is, the fact that people can own property drives a local economy. But this construct relies on a number of external forces that cannot be controlled, such as local politics (a city's eminent domain can take a person's property) and the value of the property on the open market. Measuring the influence of property ownership on a local economy (the construct) would be very difficult since there are so many confounding variables. 
	
	\item[\Gls{statisticalvalidity}] examines the extent to which conclusions derived from a statistical procedure are valid. For example, it examines whether the right statistical method was used and whether the variables meet the assumptions of that statistical test (such as sample size or distributional requirements). 
	
\end{description}

\subsection{Improving Internal and External Validity}

The best research designs are those that can assure high levels of internal and external validity. Such designs would guard against spurious correlations, inspire greater faith in the hypotheses testing, and ensure that the results drawn from a small sample are generalizable to the population at large. The internal validity of research designs and can be improved using four methods.

\begin{enumerate}
	\item \textbf{Manipulation} involves the researcher manipulating the independent variables in one or more ways (called ``treatments''), and compares the effects of the treatments against a control group where subjects do not receive the treatment. Treatments may include a new drug or different dosage of drug (for treating a medical condition), a new teaching style (for education), and so forth. This type of control can be achieved in experimental or quasi-experimental designs but not in non-experimental designs such as surveys. 
	
	\item \textbf{Elimination} relies on eliminating extraneous variables by holding them constant across treatments, such as by restricting the study to a single gender or a single socioeconomic status. 
	
	\item \textbf{Inclusion} is the process of separately estimating the effects of spurious variables on the dependent variable. As an example, consider the process of estimating the effect of gender on a marketing study. Inclusion techniques allow for greater generalizability of the study but also require substantially larger samples. 
	
	\item \textbf{Randomization} is aimed at canceling out the effects of extraneous variables through a process of random sampling. Two types of randomization are: 1) random selection, where a sample is selected randomly from a population, and 2) random assignment, where subjects selected in a non-random manner are randomly assigned to treatment groups. Randomization also improves external validity, allowing inferences drawn from the sample to be generalized to the population from which the sample is drawn; however, generalizability across populations is harder to ascertain since populations may differ on multiple dimensions and only a few of those dimensions can be controlled.
	
\end{enumerate}

\section{Summary}\label{ch06:summary}

\begin{itemize}
	\setlength{\itemsep}{0pt}
	\setlength{\parskip}{0pt}
	\setlength{\parsep}{0pt}
	
	\item In social science, our variables can be one of four different levels of measurement: nominal, ordinal, interval, or ratio.
	\item Indexes and typologies allow us to account for and simplify some of the complexities in our measures.
	
\end{itemize}
