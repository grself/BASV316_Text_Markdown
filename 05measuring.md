Defining and Measuring Concepts {#ch05:measuring}
===============================

Measurement
-----------

[r]{}[0.42]{} \[05:fig01\] ![image](gfx/05-cake){width="40.00000%"}

Measurement is important. People who have attempted to bake a cake from
scratch without measuring the ingredients will find, no doubt, that
measurement is the difference between a sweet desert and a disaster.
Just like in baking, measurement is important to a researcher.
Measurement means the process by which key facts, attributes, concepts,
and other phenomena are described. At its core, measurement is about
defining the research project’s terms in a precise and measurable way.
Of course, measurement in business research is not quite as simple as
using some predetermined or universally agreed-on tool, such as a
measuring cup, but there are some basic tenants on which most
researchers agree when it comes to measurement.

### What Do Researchers Measure?

The question of what business researchers measure can be answered by
asking what business researchers study. Researchers study a wide variety
of business and marketing concepts, like corporate
culture[@denison1990corporate], the price elasticity of
gasoline[@hughes2006evidence], employee turnover[@hom1995employee], and
automobile “lemons”[@akerlof1978market]. Each of these topics required
measurements of various types and researchers had to determine the best
way to do that. As you might have guessed, researchers will measure just
about anything that they have an interest in investigating.

In 1964, philosopher Abraham Kaplan wrote what has since become a
classic work in research methodology, *The Conduct of
Inquiry*[@kaplan2017conduct]. In his text, Kaplan describes different
categories of things that behavioral scientists observe. One of those
categories, which Kaplan called “observational terms,” is probably the
simplest to measure, and are the sorts of things that can be seen with
the naked eye simply by looking at them. They are terms that “lend
themselves to easy and confident verification.” If, for example,
researchers wanted to know how the conditions of playgrounds differ
across different neighborhoods, they could directly observe the variety,
amount, and condition of equipment at various playgrounds.

Indirect observables, on the other hand, are less straightforward to
assess. They are “terms whose application calls for relatively more
subtle, complex, or indirect observations, in which inferences play an
acknowledged part. Such inferences concern presumed connections, usually
causal, between what is directly observed and what the term signifies.”
If researchers conducted a study for which they wished to know a
person’s income, they could simply ask in an interview or a survey.
Thus, they would have observed income, even if it was only observed
indirectly. Birthplace might be another indirect observable. Researchers
can ask study participants where they were born, but chances are good
that they will not directly observe any of those people being born in
the locations they report.

Sometimes the measures that we are interested in are more complex and
more abstract than observational terms or indirect observables. Think
about concepts like ethnocentrism, the way a person judges another
person’s culture, and how measuring that concept would be very
challenging. In the same way, a concept like “bureaucracy” would be very
difficult to measure. In both cases, ethnocentrism and bureaucracy, the
theoretical notions represent ideas whose meaning is known but the
measurement of the concept may be nearly impossible. Kaplan referred to
these more abstract things as . Constructs are “not observational either
directly or indirectly” but they can be defined based on observables.

### How Do Researchers Measure?

Measurement in business research is a process. It occurs at multiple
stages of a research project: in the planning stage, in the data
collection stage, and sometimes even in the analysis stage.

As an example, imagine that the research question is: How do new college
students cope with the adjustment to college? The first problem is to
define “cope” in such a way that it can be measured. After that, the
data collection phase can be designed to measure whatever “cope” means.
After the data are collected then the analysis begins. Perhaps during
the analysis phase an unexpected facet of coping is discovered and that
may mean that the measures taken would need to be revisited to allow for
that facet. Once the analysis is complete then there are certain
decisions concerning the report. Perhaps one method of coping is
determined to be more effective than others so the report may contain a
recommendation that future research be conducted that measures just that
one method of coping. The point is that measurement considerations are
important throughout the research project.

The measurement process could also involve multiple stages. Starting
with identifying and defining key terms to determining how to observe
and measure them to assessing the quality of the measurements, there are
multiple steps involved in the measurement process. An additional step
in the measurement process involves deciding what type of data will be
collected and an appropriate analysis process for those particular types
of data elements.

Conceptualization
-----------------

One of the first steps in the measurement process is conceptualization,
which is defining the terms of the project as clearly as possible. Keep
in mind that terms mean only what the researcher determines, nothing
more and nothing less.

A *concept* is the notion or image that is conjured up when the
researcher thinks of some cluster of related observations or ideas. For
example, masculinity is a concept. A researcher thinking about that
concept may imagine some set of behaviors and perhaps even a particular
style of self presentation. Of course, not everyone will conjure up that
same set of ideas or images: in fact, there are many possible ways to
define the term. While some definitions may be more common or have more
support than others, there is not one true,
always-correct-in-all-settings definition for “masculine” and that
definition may well change over time, from culture to culture, and even
from individual to individual, as explained by George
Mosse[@george1996image]. This is why defining concepts is so important
before any data gathering begins.

It may seem unreasonable for a researcher to define a term for which
there is no single, correct definition. Unfortunately, this will be a
problem for most concepts measured in a business or marketing study.
William Clinton, the 42^d^ President of the United States, famously
stated “It depends upon what the meaning of the word ’is’ is.”[^1]
Without understanding how a researcher has defined the key concepts it
would be impossible to understand the importance of the findings.

Defining concepts is an early part of the process of measurement called
conceptualization, which involves writing out clear, concise definitions
for key concepts. Brainstorming may help to conceptualize a topic, but
it would also make sense to consult existing research and theory to see
if other scholars have already defined the concepts of interest. This
does not necessarily mean that their definitions are correct, but
understanding how concepts have been defined in the past will help with
a current project. Conceptualization is not as simple as merely applying
a definition from a dictionary, it requires careful consideration and
evaluating alternative concepts.

One important decision while conceptualizing constructs is specifying
whether they are unidimensional or multidimensional. Unidimensional
constructs are those that are expected to have a single underlying
dimension and can be measured using a single measure or test. Examples
include simple constructs such as a person’s weight, wind speed, and
even complex constructs like self-esteem (if self-esteem is
conceptualized as consisting of a single dimension, which of course, may
be unrealistic). Multidimensional constructs consist of two or more
underlying dimensions. For instance, if a person’s academic aptitude is
conceptualized as consisting of two dimension, mathematical and verbal
ability, then academic aptitude is a multidimensional construct. Each of
the underlying dimensions in this case must be measured separately by
using different tests for mathematical and verbal ability, and then
combine the two scores, possibly in a weighted manner, to create an
overall value for the academic aptitude construct.

Before moving on to the next steps in the measurement process, it would
be wise to consider one of the dangers associated with
conceptualization. While it is important to consult prior scholarly
definitions of key concepts, it would be wrong to assume that those
definitions are any more real than whatever current definitions are
generated by the researcher. It would also be wrong to assume that just
because definitions exist for some concept that the concept itself
exists beyond some abstract idea. This idea, assuming that abstract
concepts exist in some concrete way is known as reification.

To better understand reification, take a moment to think about the
concept of “family.” This concept is central to sociological thinking,
but it is an abstract term. If researchers were interested in studying
this concept, they would consult prior research to understand how the
term has been conceptualized by others. But they should also question
past conceptualizations. Today’s conceptualization of “family” would be
very different from one that was used a hundred years ago. The point is
that terms mean nothing more and nothing less than whatever definition
is assigned by the researcher. Sure, it makes sense to come to some
agreement about what various concepts mean. Without that agreement, it
would be difficult to navigate through everyday living. But at the same
time, it is important to remember that a society has assigned those
definitions and that they are no more real than any other, alternative
definition a researcher might choose to assign.

Operationalization
------------------

Once a theoretical construct is defined, indicators for measuring the
construct are defined in a process called operationalization. For
instance, if an unobservable theoretical construct such as socioeconomic
status is defined as the level of family income then it can be
operationalized using an indicator that asks respondents the question:
what is your annual family income? Given the high level of subjectivity
and imprecision inherent in social science constructs, most (except a
few demographic constructs such as age, gender, education, and income)
are measured using multiple indicators.

Indicators operate at the empirical level in contrast to constructs,
which are conceptualized at the theoretical level. The combination of
indicators at the empirical level representing a given construct is
called a , and those may be independent, dependent, mediating, or
moderating, depending on how they are employed in a research study. Also
each indicator may have several attributes (or levels) and each
attribute represent a value. For instance, a “gender” variable may have
two attributes: male or female. Likewise, a customer satisfaction scale
may be constructed to represent five attributes: “strongly
dissatisfied,” “somewhat dissatisfied,” “neutral,” “somewhat satisfied”
and “strongly satisfied.”

Variables may be quantitative (numeric) or qualitative (textual).
Quantitative data can be analyzed using techniques like regression or
structural equation modeling while qualitative data uses techniques like
coding. Note that many variables in business research are qualitative,
even when represented in a quantitative manner. For instance, a customer
satisfaction indicator with five attributes: strongly dissatisfied,
somewhat dissatisfied, neutral, somewhat satisfied, and strongly
satisfied, can assign the numbers $ 1-5 $ respectively for these five
attributes, so that we can use sophisticated statistical tools for
quantitative data analysis. However, note that the numbers are only
labels associated with respondents’ personal evaluation of their own
satisfaction, and the underlying variable (satisfaction) is still
qualitative even though it is represented numerically.

Indicators may be reflective or formative. A reflective indicator is a
measure that “reflects” an underlying construct. For example, if
religiosity is defined as a construct that measures how religious a
person is, then attending religious services may be a reflective
indicator of religiosity. A formative indicator is a measure that
“forms” or contributes to an underlying construct. Such indicators may
represent different dimensions of the construct of interest. For
instance, if religiosity is defined as composed of a belief dimension, a
devotional dimension, and a ritual dimension, then indicators chosen to
measure each of these different dimensions will be considered formative
indicators. Unidimensional constructs are measured using reflective
indicators (even though multiple reflective indicators may be used for
measuring abstruse constructs such as self-esteem), while
multidimensional constructs are measured as a formative combination of
the multiple dimensions, even though each of the underlying dimensions
may be measured using one or more reflective indicators.

It is important to keep in mind that the process of coming up with
indicators cannot be arbitrary or casual. One way to avoid taking an
overly casual approach in identifying indicators is to turn to prior
theoretical and empirical work. Theories will point to relevant concepts
and possible indicators while empirical work will detail specific
examples of how key concepts have been measured in the past. One final
important detail to think about when deciding on indicators is the
strategy you will use for data collection. A survey implies one way of
measuring concepts while field research implies a very different way.
The data-collection strategy employed will play a major role in shaping
how concepts are operationalized.

Measurement Quality
-------------------

The previous section examined some of the difficulties with measuring
constructs. What makes the task more challenging is that sometimes these
constructs are imaginary concepts (i.e., they don’t exist in reality),
and multi-dimensional (in which case, we have the added problem of
identifying their constituent dimensions). Hence, it is not adequate
just to measure constructs using any scale, the scales must be tested to
ensure that:

1.  they measure the construct consistently and precisely (i.e., the
    scales are “reliable”) and

2.  they actually measure the construct being investigated (i.e., the
    scales are “valid”).

Reliability, the consistency of a measure, and validity, the efficacy of
a measure, are the two yardsticks against which the accuracy of
measurements are evaluated in scientific research. A measure can be
reliable but not valid if it is measuring consistently but it is the
wrong construct. Likewise, a measure can be valid but not reliable if it
is measuring the right construct but not doing so in a consistent
manner. Using the analogy of a shooting target, as shown in Figure
\[05:fig03\], a measure that is both reliable and valid is like a group
that is tightly clustered near the center of the target. A measure that
is reliable but not valid is like a group that is tightly clustered but
off-center. A measure that is valid but not reliable is a group that is
widely scattered but centered. Finally, a measure that is neither
reliable nor valid is like a group that is widely scattered and
off-center.

### Reliability

“...is the extent to which measurements are repeatable – when different
persons perform the measurements, on different occasions, under
different conditions, with supposedly alternative instruments which
measure the same thing.”[@drost2011validity]

Any score obtained by a measuring instrument (the observed score) is
composed of both the “true” score, which is the score that a person
would have received if the measurement were perfectly accurate, and the
“error” in the measurement process. Imagine a simple example, a bathroom
scale. If a person’s true weight were $ 150 $ pounds then, ideally, the
scale would read $ 150 $ every time that person stepped on the scale.
The scale’s reliability is the consistency of its output from one day to
the next. If a person stepped on the scale one day and it read $ 160 $
but the next day it read $ 140 $ then the scale would not be a reliable
instrument.

There are two types of reliability errors that researchers need to
understand. First is systematic error, one that is caused by the system
and is predictable. For example, the if the bathroom scale mentioned
above constantly read five pounds heavy that would be an error, but it
would be one that is consistent and could be corrected in the research
data. That is an example of a systematic error. The second type of error
is a random error. If the bathroom scale were accurate but the person
reading it one day read $ 151 $ and the next as $ 149 $ then that would
be a random error. Random errors cannot be corrected but tend to cancel
out due to the random nature of the error (sometimes the reading will be
a bit high and other times low), especially if there are many data
points.

Unreliable measurements in business research could be for several
reasons. One is the researcher’s subjectivity. For example, if employee
morale in a firm is being measured by watching whether the employees
smile at each other, whether they make jokes, and so forth, then
different observers may infer different measures of morale if they are
watching the employees on a very busy day (when they have no time to
joke or chat) or a light day (when they are more jovial or chatty). Two
observers may also infer different levels of morale on the same day,
depending on what they view as a joke and what is not. “Observation” is
a qualitative measurement technique.

Sometimes, reliability may be improved by using quantitative measures.
Counting the number of grievances filed over one month as a measure of
(the inverse of) morale. Of course, grievances may or may not be a valid
measure of morale, but it is less subject to human subjectivity, and
therefore more reliable.

A second source of unreliable observation is asking imprecise or
ambiguous questions. For instance, if people are asked to report their
salary some may state a monthly salary, some an annual salary, and some
even an hourly wage. Thus, the resulting observations will be divergent
and unreliable.

A third source of unreliability is asking questions about issues that
respondents are not very familiar with or care about, such as asking an
American college graduate about Canada’s relationship with Slovenia or
asking a Chief Executive Officer to rate the effectiveness of his
company’s technology strategy (which was likely delegated to a
technology executive).

To improve reliability, start by replacing subjective data collection
techniques (observation) with those that are more objective
(questionnaire), ask respondents only questions that they may know or
care about, avoid ambiguous items (e.g. clearly indicate annual salary),
and simplify the wording in indicators. While these strategies can
improve the reliability of measurements, instruments must still be
tested for reliability using techniques like the following.

-   **Inter-rater reliability**. Inter-rater reliability, also called
    inter-observer reliability, is a measure of consistency between two
    or more independent raters (observers) of the same construct.
    Usually, this is assessed in a pilot study and can be done in two
    ways, depending on the level of measurement being used. If the
    measure is categorical, a set of all categories is defined, raters
    check off which category each observation falls in, and the
    percentage of agreement between the raters is used as an estimate of
    inter-rater reliability. For instance, if there are two raters
    rating $ 100 $ observations into one of three possible categories,
    and their ratings match for $ 75\% $ of the observations, then
    inter-rater reliability is $ 0.75 $. If the measure is interval or
    ratio scaled (e.g., classroom activity is being measured once every
    five minutes by two raters on one to seven scale), then a simple
    correlation between measures from the two raters can also serve as
    an estimate of inter-rater reliability.

-   **Test-retest reliability**. Test-retest reliability is a measure of
    consistency between two measurements (tests) of the same construct
    administered to the same sample at two different points in time. If
    the observations have not changed substantially between the two
    tests, then the measure is reliable. The correlation in observations
    between the two tests is an estimate of test-retest reliability.
    Note here that the time interval between the two tests is critical.
    Generally, the longer is the time gap, the greater is the chance
    that the two observations may change during this time (due to random
    error), and the lower will be the test-retest reliability.

-   **Split-half reliability**. Split-half reliability is a measure of
    consistency between two halves of a construct measure. For instance,
    if you have a ten-item measure of a given construct, randomly split
    those ten items into two sets of five (unequal halves are allowed if
    the total number of items is odd), and administer the entire
    instrument to a sample of respondents. Then, calculate the total
    score for each half for each respondent, and the correlation between
    the total scores in each half is a measure of split-half
    reliability. The longer the instrument, the more likely it is that
    the two halves of the measure will be similar (since random errors
    are minimized as more items are added), and hence, this technique
    tends to systematically overestimate the reliability of longer
    instruments.

-   **Internal consistency reliability**. Internal consistency
    reliability is a measure of consistency between different items of
    the same construct. If a multiple-item construct measure is
    administered to respondents, the extent to which respondents rate
    those items in a similar manner is a reflection of internal
    consistency. This reliability can be estimated in terms of average
    inter-item correlation, average item-to-total correlation, or more
    commonly, *Cronbach’s alpha*.

### Validity

is concerned with the meaningfulness of research results. In brief, does
the research actually measure what it was purported to measure? For
example, does the Scholastic Aptitude Test (SAT) actually predict the
likelihood of a high school student successfully completing
college?[@drost2011validity] There are numerous types of validity found
in the literature, but they generally form two large groups: Measurement
Validity (the measurement should accurately reflect the construct) and
Hypothesis Validity (the hypotheses should accurately reflect the
construct).

#### Measurement Validity

The *theoretical* assessment of validity focuses on how well an abstract
construct is translated into an operational measure, which is called ,
and divided into two sub-types: face and content validity. Translational
validity is typically assessed using a panel of expert judges who rate
each item (indicator) on how well it fits the conceptual definition of
that construct along with a qualitative technique called *Q-method*, as
explained by Pnina Shinebourne[@shinebourne2009using].

The *empirical* assessment of validity examines how well a given measure
relates to one or more external criterion, based on empirical
observations. This type of validity is called , which is divided into
four sub-types: convergent, discriminant, concurrent, and predictive.
While translation validity examines whether a measure is a good
reflection of its underlying construct, criterion-related validity
examines whether a given measure behaves the way it should, given the
theory of that construct. The distinction between theoretical and
empirical assessment of validity is illustrated in Figure \[05:fig04\].
However, both approaches are needed to adequately ensure the validity of
measures in business research.

##### Translational Validity

:   refers to whether an indicator seems to be a reasonable measure of
    its underlying construct “on its face.” For instance, the frequency
    of attendance at religious services seems to make sense as an
    indication of a person’s religiosity without a lot of explanation.
    Hence this indicator has face validity. However, if we were to
    suggest how many books were checked out of an office library as a
    measure of employee morale, then such a measure would probably lack
    face validity because it does not seem to make much sense.
    Interestingly, some of the popular measures used in organizational
    research appear to lack face validity. For instance, absorptive
    capacity of an organization (how much new knowledge can it
    assimilate for improving organizational processes) has often been
    measured by research and development intensity (, R&D expenses
    divided by gross revenues). Research that includes constructs that
    are highly abstract or are hard to conceptually separate from each
    other (, compassion and empathy), it may be worthwhile to use a
    panel of experts to evaluate the face validity of the measures.

:   is an assessment of how well a measure matches the content domain of
    the construct being measured. For instance, to measure the construct
    “satisfaction with restaurant service,” then the content domain
    should include the quality of food, courtesy of wait staff, duration
    of wait, and the overall ambiance of the restaurant (i.e., whether
    it is noisy, smoky, etc.). The project should measure the extent to
    which a restaurant patron is satisfied with the quality of food,
    courtesy of wait staff, the length of wait, and the restaurant’s
    ambiance. Of course, this approach assumes the researcher can create
    a detailed description of the content domain, which may be difficult
    for complex constructs such as self-esteem or intelligence. As with
    face validity, an expert panel of judges may be employed to examine
    content validity of constructs.

##### Criterion-Related Validity

:   refers to the closeness with which a measure, or group of measures,
    relates to (or converges on) the construct that it is purported to
    measure. Convergent validity can be established by comparing the
    observed values of one indicator with those of other indicators to
    attempt to find high correlation between those indicators. Compare
    this to discriminant validity.

:   refers to the degree to which a measure does not measure (or
    discriminates from) other constructs that it is not supposed to
    measure. Usually, convergent validity and discriminant validity are
    assessed jointly for a set of related constructs. For example, if an
    organization’s knowledge is related to its performance then a
    measure of organizational knowledge must actually measure
    organizational knowledge (convergent validity) and not
    organizational performance (discriminant validity). Discriminant
    validity is established by demonstrating that indicators of one
    construct are dissimilar from (i.e., have low correlation with)
    other constructs.

:   examines how well a measure of one outcome relates to another
    outcome that is presumed to occur simultaneously. For instance, do
    students’ scores in a calculus class correlate well with their
    scores in a linear algebra class? Since both are mathematics classes
    it would be presumed that there is high concurrent validity between
    scores in those classes.

:   is the degree to which a measure successfully predicts a future
    outcome. For example, a standardized test score (, *Scholastic
    Aptitude Test*) can be used to predict a student’s academic success
    in college. Concurrent and predictive validity are not often
    considered in empirical business research.

#### Hypothesis Validity

In general, four types of hypothesis validity are referred to in the
literature.

:   examines whether the observed change in a dependent variable is
    caused by a corresponding change in hypothesized independent
    variable and not by variables extraneous to the research context.
    This is sometimes called “causality” and it requires three
    conditions:

    1.  covariation of cause and effect (if cause happens then effect
        also happens and if cause does not happen effect does not
        happen)

    2.  temporal precedence (cause must precede effect in time)

    3.  lack of plausible alternative explanation (or spurious
        correlation).

    Certain research designs, such as laboratory experiments, are strong
    in internal validity since researchers can manipulate the
    independent variable (cause) via a treatment and observe the effect
    (dependent variable) of that treatment after a certain point in time
    while controlling for the effects of extraneous variables. Other
    designs, such as field surveys, are poor in internal validity
    because researchers cannot manipulate the independent variable
    (cause) and because cause and effect are measured at the same point
    in time which defeats temporal precedence making it equally likely
    that the effect actually brought about the presumed cause rather
    than the reverse.

:   refers to whether the observed associations can be generalized from
    the sample to the population (population validity), or to entities
    outside the population (ecological validity). For example, if
    results drawn from a sample of financial firms in the United States
    can be generalized to the population of all financial firms it would
    have strong population validity and if to other types of firms it
    would have strong ecological validity. Survey research, where data
    are sourced from a wide variety of individuals, firms, or other
    units of analysis, tends to have broader generalizability than
    laboratory experiments where artificially contrived treatments and
    strong control over extraneous variables render the findings less
    generalizable to real-life settings where treatments and extraneous
    variables cannot be controlled.

:   examines how well a given measurement scale is measuring the
    theoretical construct that it is designed to measure. One frequent
    problem with construct validity is simply defining the construct in
    such a way that it is measurable. As one example, “property
    ownership” is a construct of a market economy explained by Robert
    Reich[@reich2016saving]. That is, the fact that people can own
    property drives a local economy. But this construct relies on a
    number of external forces that cannot be controlled, such as local
    politics (a city’s eminent domain can take a person’s property) and
    the value of the property on the open market. Measuring the
    influence of property ownership on a local economy (the construct)
    would be very difficult since there are so many confounding
    variables.

:   examines the extent to which conclusions derived from a statistical
    procedure are valid. For example, it examines whether the right
    statistical method was used and whether the variables meet the
    assumptions of that statistical test (such as sample size or
    distributional requirements).

### Improving Internal and External Validity

The best research designs are those that can assure high levels of
internal and external validity. Such designs would guard against
spurious correlations, inspire greater faith in the hypotheses testing,
and ensure that the results drawn from a small sample are generalizable
to the population at large. The internal validity of research designs
and can be improved using four methods.

1.  **Manipulation** involves the researcher manipulating the
    independent variables in one or more ways (called “treatments”), and
    compares the effects of the treatments against a control group where
    subjects do not receive the treatment. Treatments may include a new
    drug or different dosage of drug (for treating a medical condition),
    a new teaching style (for education), and so forth. This type of
    control can be achieved in experimental or quasi-experimental
    designs but not in non-experimental designs such as surveys.

2.  **Elimination** relies on eliminating extraneous variables by
    holding them constant across treatments, such as by restricting the
    study to a single gender or a single socioeconomic status.

3.  **Inclusion** is the process of separately estimating the effects of
    spurious variables on the dependent variable. As an example,
    consider the process of estimating the effect of gender on a
    marketing study. Inclusion techniques allow for greater
    generalizability of the study but also require substantially larger
    samples.

4.  **Randomization** is aimed at canceling out the effects of
    extraneous variables through a process of random sampling. Two types
    of randomization are: 1) random selection, where a sample is
    selected randomly from a population, and 2) random assignment, where
    subjects selected in a non-random manner are randomly assigned to
    treatment groups. Randomization also improves external validity,
    allowing inferences drawn from the sample to be generalized to the
    population from which the sample is drawn; however, generalizability
    across populations is harder to ascertain since populations may
    differ on multiple dimensions and only a few of those dimensions can
    be controlled.

Summary {#ch06:summary}
-------

-   In social science, our variables can be one of four different levels
    of measurement: nominal, ordinal, interval, or ratio.

-   Indexes and typologies allow us to account for and simplify some of
    the complexities in our measures.

[^1]: This was widely reported in the press and can be easily found
    on-line, including YouTube videos of him making that statement.
